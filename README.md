# Detecting Market Regimes with a Multi-Feature Hidden Markov Model

## The Model

A Gaussian Hidden Markov Model assumes that observable market data is generated by a latent state process — the "regime" — that transitions between $K$ discrete states according to a first-order Markov chain. Each state $k$ emits observations from its own multivariate normal distribution $\mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$. The model parameters are estimated via the Baum-Welch algorithm (EM), and the number of states is selected by minimizing the Bayesian Information Criterion.

The pipeline has five stages: data collection, feature engineering, PCA, HMM fitting with BIC selection, and diagnostics. Each is explained below with the corresponding code.

---

## The Start

```python
import warnings; warnings.filterwarnings('ignore')
import os, json, numpy as np, pandas as pd, yfinance as yf
import matplotlib; matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from hmmlearn.hmm import GaussianHMM

OUT = 'C:/Users/UsuarioHP/Desktop/hmm_output'
TICKERS = ['SPY','IWM','HYG','LQD']
K_RANGE = range(3,9)   # BIC will select from K=3..8
SEEDS = 5              # random restarts per K (EM finds local optima)
```

`hmmlearn` provides the Gaussian HMM implementation with EM training. `sklearn` handles standardization and PCA. Financial literature and similar repositories suggest markets exhibit 3–8 distinct behavioral modes. We use BIC as it penalizes unnecessary complexity.

---

## Step 1 — Data Collection

```python
def collect():
    etf = yf.download(TICKERS, start='2005-01-01', auto_adjust=False, progress=False)
    cl, hi, lo = etf['Close'], etf['High'], etf['Low']

    vix   = yf.download('^VIX',   start='2005-01-01', auto_adjust=True, progress=False)['Close']
    vix3m = yf.download('^VIX3M', start='2005-01-01', auto_adjust=True, progress=False)['Close']

    df = pd.DataFrame(index=cl.index)
    df['SPY'], df['SPY_High'], df['SPY_Low'] = cl['SPY'], hi['SPY'], lo['SPY']
    df['IWM'], df['HYG'], df['LQD'] = cl['IWM'], cl['HYG'], cl['LQD']
    df['VIX'], df['VIX3M'] = vix, vix3m
    df = df.ffill(limit=3).dropna()
    return df
```

Six data sources, each capturing a different dimension of the market:

| Source | What It Captures | Why |
|--------|-----------------|-----|
| **SPY** | Large-cap equities | Anchor — direction and volatility of the broad market |
| **IWM** | Small-cap equities | More cyclically sensitive; diverges from SPY during risk-off |
| **HYG / LQD** | High-yield / IG bonds | Log price ratio isolates credit risk |
| **VIX / VIX3M** | 30d / 3m implied vol | Ratio reveals vol term structure (contango vs backwardation) |

We need OHLC for SPY because intraday range is one of the features. HYG inception (April 2007) is the binding constraint on sample start. After combining and dropping NaN: **~4,550 daily observations** through February 2026.

---

## Step 2 — Feature Engineering

We construct 29 features spanning seven categories. Each feature should have a different distribution under different market regimes.

```python
def features(df):
    f = pd.DataFrame(index=df.index)
    sr, ir = df['SPY'].pct_change(), df['IWM'].pct_change()
    rv = lambda s,w: s.rolling(w).std() * np.sqrt(252)

    # Trend (3): MA flags + distance from 200d MA
    f['SPY_Above50D']  = (df['SPY'] > df['SPY'].rolling(50).mean()).astype(int)
    f['SPY_Above200D'] = (df['SPY'] > df['SPY'].rolling(200).mean()).astype(int)
    f['SPY_Dist_MA200'] = df['SPY'] / df['SPY'].rolling(200).mean() - 1

    # Credit (1): log price ratio isolates spread changes
    f['Credit_Spread'] = np.log(df['HYG']) - np.log(df['LQD'])

    # Multi-horizon returns (8): daily, 21d, 63d, 126d for SPY + IWM
    for t, r, n in [('SPY',sr,'SPY'), ('IWM',ir,'IWM')]:
        f[f'{n}_Daily_Ret']=r; f[f'{n}_21D_Ret']=df[t].pct_change(21)
        f[f'{n}_63D_Ret']=df[t].pct_change(63); f[f'{n}_126D_Ret']=df[t].pct_change(126)

    # Relative performance (2): large vs small cap tilt
    f['SPY_vs_IWM_21D'] = f['SPY_21D_Ret'] - f['IWM_21D_Ret']
    f['SPY_vs_IWM_63D'] = f['SPY_63D_Ret'] - f['IWM_63D_Ret']

    # Realized vol (4): annualized rolling σ at 21d and 63d
    f['SPY_21D_RVol']=rv(sr,21); f['SPY_63D_RVol']=rv(sr,63)
    f['IWM_21D_RVol']=rv(ir,21); f['IWM_63D_RVol']=rv(ir,63)

    # Microstructure (2): intraday range + vol-of-vol
    f['SPY_Range']  = (df['SPY_High'] - df['SPY_Low']) / df['SPY']
    f['SPY_VoV_20'] = f['SPY_21D_RVol'].rolling(20).std()

    # VIX (5): level, Δ1d, Δ5d, implied/realized ratio, term structure
    f['VIX_Level']=df['VIX']; f['VIX_1D_Chg']=df['VIX'].diff(1); f['VIX_5D_Chg']=df['VIX'].diff(5)
    f['VIX_to_RVol'] = df['VIX'] / (f['SPY_21D_RVol'] * 100)
    f['VIX3M_VIX']   = df['VIX3M'] / df['VIX']

    # Drawdowns (2): distance from 126d high
    f['SPY_126D_DD'] = df['SPY'] / df['SPY'].rolling(126, min_periods=1).max() - 1
    f['IWM_126D_DD'] = df['IWM'] / df['IWM'].rolling(126, min_periods=1).max() - 1

    # Cross-asset (2): rolling correlation and beta
    f['SPY_IWM_63D_Corr'] = sr.rolling(63).corr(ir)
    f['IWM_Beta_SPY'] = sr.rolling(63).cov(ir) / sr.rolling(63).var()

    return f.dropna()
```

**Multi-horizon returns** (daily through 126d) let the model distinguish fast shocks from sustained trends. **Intraday range** $(H - L)/C$ captures within-day stress that close-to-close returns miss. **Vol-of-vol** measures whether volatility itself is unstable — a precursor to regime transitions. **VIX3M/VIX** exposes the volatility term structure: contango (> 1) = calm, backwardation (< 1) = stress. **Credit spread** as log(HYG) − log(LQD) gives a daily tradeable proxy for credit risk.

> **Output:** 29 features × 4,547 observations (Jan 2008 – Feb 2026).

---

## Step 3 — PCA

Many of the 29 features are correlated. Feeding correlated features into a diagonal-covariance HMM violates the conditional independence assumption and produces degenerate states.

![Feature Correlation Matrix](plots/plot1_correlation.png)

*51 feature pairs with |r| > 0.7. Without dimensionality reduction, diagonal covariance would be inappropriate.*

PCA rotates the feature space into orthogonal components. We standardize first, then keep enough components to explain 95% of variance:

```python
def do_pca(feat, target=0.95):
    X = feat.replace([np.inf, -np.inf], np.nan).dropna()
    sc = StandardScaler()
    Xs = sc.fit_transform(X.values)        # z-score: (x - μ) / σ

    pca = PCA()
    Xp = pca.fit_transform(Xs)             # eigendecomposition of cov matrix

    n = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= target) + 1
    return Xp[:, :n], X.index, Xs, pca, n, X.columns
```

![PCA Scree Plot](plots/plot2_pca_scree.png)

*15 components capture 95.8% of variance. The remaining 14 contribute noise.*

Since PCA components are uncorrelated by construction, diagonal covariance in PCA space is equivalent to full covariance in raw feature space — but with $K \times 2d$ parameters instead of $K \times d(d+1)/2$. For $K = 8$, $d = 15$: 240 emission parameters vs. 960.

---

## Step 4 — HMM Fitting and BIC Selection

We fit a Gaussian HMM for each candidate $K$ and select by BIC:

$$\text{BIC}(K) = -2 \ln L_K + p_K \ln T$$

where $p_K = (K-1) + K(K-1) + 2Kd$ counts the free parameters, $L_K$ is the maximized likelihood, and $T$ is the sample size.

```python
def fit_hmm(X):
    T, d = X.shape

    def bic(m, X):
        K = m.n_components; logL = m.score(X)
        p = (K-1) + K*(K-1) + K*2*d
        return -2*logL + p*np.log(T), logL

    res = []; best = {'bic': np.inf}
    for K in K_RANGE:
        bk = {'bic': np.inf}
        for s in range(SEEDS):             # multiple restarts (EM → local optima)
            m = GaussianHMM(n_components=K, covariance_type='diag',
                            n_iter=1000, random_state=42+s)
            m.fit(X)
            b, l = bic(m, X)
            if b < bk['bic']: bk = {'bic':b, 'm':m, 'logL':l}
        res.append((K, bk['bic'], bk['logL']))
        if bk['bic'] < best['bic']: best = bk
    return best['m'], best['m'].predict(X), res
```

![BIC Selection](plots/plot3_bic.png)

| K | BIC | log L | Parameters |
|---|-----|-------|------------|
| 3 | 171,316 | −85,245 | 98 |
| 4 | 165,511 | −82,187 | 135 |
| 5 | 161,374 | −79,954 | 174 |
| 6 | 158,167 | −78,178 | 215 |
| 7 | 155,851 | −76,839 | 258 |
| **8** | **154,073** | **−75,760** | **303** |

BIC selected **K = 8**: the market exhibits at least 8 statistically distinguishable behavioral modes.

---

## Step 5 — State Diagnostics

HMM state labels are arbitrary integers. We give them meaning by computing average SPY return, realized vol, drawdown, and Sharpe ratio per state:

```python
def diagnose(df, feat, states, idx, model):
    ret = df['SPY'].pct_change().reindex(idx).values
    rv  = feat['SPY_21D_RVol'].reindex(idx).values
    dd  = feat['SPY_126D_DD'].reindex(idx).values

    for s in range(model.n_components):
        mask = (states == s)
        r = ret[mask]
        ann_ret = np.nanmean(r) * 252
        ann_vol = np.nanmean(rv[mask])
        sharpe  = ann_ret / (np.nanstd(r) * np.sqrt(252))
```

![State Diagnostics](plots/plot4_diagnostics.png)

| State | Days | % | Ann Ret | Ann Vol | Sharpe | Avg DD | Interpretation |
|-------|------|---|---------|---------|--------|--------|---------------|
| S0 | 646 | 14.2% | +32.9% | 11.1% | 3.33 | −1.1% | Strong bull |
| S7 | 586 | 12.9% | +27.7% | 10.4% | 3.42 | −0.6% | Quiet bull |
| S3 | 612 | 13.5% | +25.3% | 14.2% | 1.63 | −1.8% | Moderate bull |
| S2 | 599 | 13.2% | +22.2% | 8.2% | 2.68 | −0.5% | Steady low-vol bull |
| S4 | 373 | 8.2% | +11.3% | 22.5% | 0.53 | −10.2% | High-vol recovery |
| S6 | 950 | 20.9% | +10.4% | 14.6% | 0.69 | −3.6% | Transition / sideways |
| S5 | 345 | 7.6% | −23.2% | 43.8% | −0.46 | −20.8% | Crisis |
| S1 | 436 | 9.6% | −51.1% | 23.6% | −2.15 | −11.3% | Sustained bear |

The Sharpe spread between the best state (3.42) and the worst (−2.15) is over 5.5. The 8 states cluster into three actionable tiers:

- **Full exposure** — S0, S2, S3, S7 (Sharpe > 1.6, vol < 15%)
- **Reduced exposure** — S4, S6 (positive returns but elevated vol or low Sharpe)
- **Defensive** — S1, S5 (negative returns, high vol)

---

## Step 6 — Filtered Probabilities

Viterbi uses the entire time series (including future data). For real-time regime estimation we use the forward algorithm — filtered probabilities condition only on data up to time $t$:

$$P(S_t = k \mid y_{1:t}) = \frac{\alpha_t(k)}{\sum_j \alpha_t(j)}$$

```python
def filtered(model, X):
    ls = np.log(np.maximum(model.startprob_, 1e-300))
    lt = np.log(np.maximum(model.transmat_,  1e-300))
    lb = model._compute_log_likelihood(X)
    T, K = lb.shape

    la = np.empty((T, K))
    la[0] = ls + lb[0]
    la[0] -= np.logaddexp.reduce(la[0])

    for t in range(1, T):
        tr = la[t-1][:, None] + lt
        la[t] = np.logaddexp.reduce(tr, axis=0) + lb[t]
        la[t] -= np.logaddexp.reduce(la[t])

    p = np.exp(la[-1])
    return p / p.sum()
```

> **Current regime (2026-02-21):** S6 (52.6%) / S3 (47.4%). The model is near a boundary between "transition" (+10.4%/yr, Sharpe 0.69) and "moderate bull" (+25.3%/yr, Sharpe 1.63).

---

## Regime Timeline

![Regime Timeline](plots/plot5_regimes.png)

*SPY price with colored regime bands (green = bullish, yellow = transition, red = bearish/crisis). Bottom panels: 21-day realized vol and 126-day drawdown. Major stress episodes (GFC, EU crisis, COVID, 2022 bear) are correctly captured.*

---

## Installation

```bash
git clone https://github.com/YOUR_USERNAME/hmm-regime-detection.git
cd hmm-regime-detection
pip install -r requirements.txt
python hmm_v7_final.py
```

## Requirements

```
numpy>=1.24
pandas>=2.0
yfinance>=0.2.31
scikit-learn>=1.3
hmmlearn>=0.3.0
matplotlib>=3.7
```

## Output

The script saves to the output directory:

| File | Description |
|------|-------------|
| `plot1_correlation.png` | 29×29 feature correlation matrix |
| `plot2_pca_scree.png` | PCA variance explained |
| `plot3_bic.png` | BIC model selection curve |
| `plot4_diagnostics.png` | Return / vol / Sharpe / drawdown per state |
| `plot5_regimes.png` | Full regime timeline on SPY |
| `hmm_v7_summary.json` | Model parameters and current regime |
| `regimes_v7.csv` | Daily state assignments |

## Repository Structure

```
hmm-regime-detection/
├── hmm_v7_final.py
├── requirements.txt
├── README.md
└── plots/
    ├── plot1_correlation.png
    ├── plot2_pca_scree.png
    ├── plot3_bic.png
    ├── plot4_diagnostics.png
    └── plot5_regimes.png
```

## References

- Rabiner, L. R. (1989). *A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.* Proceedings of the IEEE, 77(2).
- Hamilton, J. D. (1989). *A New Approach to the Economic Analysis of Nonstationary Time Series and the Business Cycle.* Econometrica, 57(2).
- Borst, D. (2025). *Detecting Market Regimes: Hidden Markov Model.* Medium.
- Schwarz, G. (1978). *Estimating the Dimension of a Model.* Annals of Statistics, 6(2).

## License

MIT
